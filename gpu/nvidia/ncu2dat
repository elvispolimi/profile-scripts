#!/usr/bin/env python3

import csv
import sys
import argparse
import statistics
import math
from collections import defaultdict

parser = argparse.ArgumentParser(
    description="Generate gnuplot data file from an NVIDIA Nsight profiler CSV file. Reads the CSV from stdin and writes the result to stdout."
)
parser.add_argument(
    "--template",
    "-t",
    type=argparse.FileType("r"),
    default=None,
    help="Template file containing a python format string to be instantiated with resulting values",
)

parser.add_argument(
    "--input",
    "-i",
    type=argparse.FileType("r"),
    default=None,
    help="Input file",
)
parser.add_argument(
    "--kernels",
    "-k",
    required=True,
    help="Comma-separated list of kernel name substrings to include",
)
parser.add_argument(
    "--raw",
    action="append",
    type=argparse.FileType("r"),
    default=None,
    help="Raw NCU report from --page raw --section SpeedOfLight_HierarchicalSingleRooflineChart (repeatable)",
)
parser.add_argument(
    "--warmup",
    type=int,
    default=0,
    help="Number of initial raw runs to discard when averaging",
)
parser.add_argument(
    "--tensor-flop-factor",
    type=float,
    default=512.0,
    help="FLOPs per tensor instruction (architecture/precision dependent)",
)
parser.add_argument(
    "--debug-ai",
    action="store_true",
    help="Print per-kernel AI inputs (fp_flop, l1/l2/dram bytes) to stderr",
)
parser.add_argument(
    "--debug-ai-raw",
    action="store_true",
    help="Print raw counters used for AI/FLOP computations to stderr (raw inputs only)",
)


def elapsed_s(d):
    return d["sm__cycles_elapsed.avg"] / float(d["sm__cycles_elapsed.avg.per_second"])


def scalar_flop(d):
    return (
        2 * d["sm__sass_thread_inst_executed_op_dfma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_dmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_dadd_pred_on.sum"]
        + 2 * d["sm__sass_thread_inst_executed_op_ffma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_fmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_fadd_pred_on.sum"]
        + 2 * d["sm__sass_thread_inst_executed_op_hfma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_hmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_hadd_pred_on.sum"]
    )


def tensor_flop(d, tensor_flop_factor):
    return tensor_flop_factor * d["sm__inst_executed_pipe_tensor.sum"]

def dp_flop(d):
    return (
        2 * d["sm__sass_thread_inst_executed_op_dfma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_dmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_dadd_pred_on.sum"]
    )


def fp_flop(d, tensor_flop_factor):
    return (
        2 * d["sm__sass_thread_inst_executed_op_ffma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_fmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_fadd_pred_on.sum"]
        + 2 * d["sm__sass_thread_inst_executed_op_hfma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_hmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_hadd_pred_on.sum"]
        + tensor_flop(d, tensor_flop_factor)
    )


def flop(d, tensor_flop_factor):
    return fp_flop(d, tensor_flop_factor) + dp_flop(d)


def thread_instructions(d):
    return d["smsp__thread_inst_executed.sum"] / 32.0


def l1_global_transactions(d):
    return (
        d["l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum"]
        + d["l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum"]
    )


def l1_shared_transactions(d):
    return (
        d["l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum"]
        + d["l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum"]
    )


def l2_transactions(d):
    return (
        d["lts__t_sectors_op_read.sum"]
        + d["lts__t_sectors_op_atom.sum"] * 2
        + d["lts__t_sectors_op_red.sum"] * 2
        + d["lts__t_sectors_op_write.sum"]
    )


def dram_transactions(d):
    return d["dram__sectors_read.sum"] + d["dram__sectors_write.sum"]


def warp_instructions(d):
    return d["smsp__inst_executed.sum"]


def l1_total_32B_transactions(d):
    return l1_global_transactions(d) + 4 * l1_shared_transactions(d)


def warp_global_ld_st_instructions(d):
    return d["smsp__inst_executed_op_global_ld.sum"]


def warp_shared_ld_st_instructions(d):
    return (
        d["smsp__inst_executed_op_shared_ld.sum"]
        + d["smsp__inst_executed_op_shared_st.sum"]
    )


def dram_bytes(d):
    return d["dram__bytes.sum"]


def l2_bytes(d):
    return d["lts__t_bytes.sum"]


def l1_bytes(d):
    return d["l1tex__t_bytes.sum"]


def fp_instructions(d):
    return (
        d["sm__sass_thread_inst_executed_op_dfma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_dmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_dadd_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_ffma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_fmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_fadd_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_hfma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_hmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_hadd_pred_on.sum"]
        + d["sm__inst_executed_pipe_tensor.sum"]
    )


def int_instructions(d):
    return d["sm__sass_thread_inst_executed_op_integer_pred_on.sum"]


def cf_instructions(d):
    return d["sm__sass_thread_inst_executed_op_control_pred_on.sum"]


def threadcomm_instructions(d):
    return d["sm__sass_thread_inst_executed_op_inter_thread_communication_pred_on.sum"]


def mem_instructions(d):
    return d["sm__sass_thread_inst_executed_op_memory_pred_on.sum"]


def misc_instructions(d):
    return (
        d["sm__sass_thread_inst_executed_op_bit_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_conversion_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_misc_pred_on.sum"]
    )


def tot_instructions(d):
    return (
        fp_instructions(d)
        + int_instructions(d)
        + cf_instructions(d)
        + threadcomm_instructions(d)
        + mem_instructions(d)
        + misc_instructions(d)
    )


def instmix_pct(d):
    tot = tot_instructions(d)
    keys = [
        "fp_instructions_pct",
        "int_instructions_pct",
        "mem_instructions_pct",
        "cf_instructions_pct",
        "threadcomm_instructions_pct",
        "misc_instructions_pct",
    ]
    values = [
        fp_instructions(d) / tot * 100,
        int_instructions(d) / tot * 100,
        mem_instructions(d) / tot * 100,
        cf_instructions(d) / tot * 100,
        threadcomm_instructions(d) / tot * 100,
        misc_instructions(d) / tot * 100,
    ]
    # Apply largest remainder method to compensate for rounding errors
    components = [math.modf(v) for v in values]
    integral = [v[1] for v in components]
    fractional = list(sorted(enumerate(v[0] for v in components), key=lambda x: x[1]))
    while sum(integral) < 100:
        idx, _ = fractional.pop()
        integral[idx] += 1
    return {keys[i]: integral[i] for i in range(len(keys))}


def instmix(d):
    return {
        "fp_instructions": fp_instructions(d),
        "int_instructions": int_instructions(d),
        "mem_instructions": mem_instructions(d),
        "cf_instructions": cf_instructions(d),
        "threadcomm_instructions": threadcomm_instructions(d),
        "misc_instructions": misc_instructions(d),
        "tot_instructions": tot_instructions(d),
    }


def occupancy(d):
    peak = d["sm__maximum_warps_per_active_cycle_pct"]
    sustained = d["sm__warps_active.avg.pct_of_peak_sustained_active"]
    return {
        "peak_occupancy_margin_pct": peak - sustained,
        "sustained_occupancy_pct": sustained,
    }


def efficiency(d):
    return {"efficiency": thread_instructions(d) / warp_instructions(d) * 100}


def performance(d, tensor_flop_factor):
    return {"flops": flop(d, tensor_flop_factor) / (elapsed_s(d) * 1000000000)}


def canonicalize(d, tensor_flop_factor):
    return {
        # Timing
        "elapsed_s": elapsed_s(d),
        # Bandwidth
        "dram_bytes": dram_bytes(d),
        "l2_bytes": l2_bytes(d),
        "l1_bytes": l1_bytes(d),
        # FLOP
        "flop": flop(d, tensor_flop_factor),
        "scalar_flop": scalar_flop(d),
        "fp_flop": fp_flop(d, tensor_flop_factor),
        "dp_flop": dp_flop(d),
        "tensor_flop": tensor_flop(d, tensor_flop_factor),
        # 1. Instruction Intensity and Performance
        "thread_instructions": thread_instructions(d),
        "l1_global_transactions": l1_global_transactions(d),
        "l1_shared_transactions": l1_shared_transactions(d),
        "l2_transactions": l2_transactions(d),
        "dram_transactions": dram_transactions(d),
        # 2. Thread Predication
        "warp_instructions": warp_instructions(d),
        # 3. Global Memory Pattern Walls
        "l1_total_32B_transactions": l1_total_32B_transactions(d),
        "warp_global_ld_st_instructions": warp_global_ld_st_instructions(d),
        # 4. Shared Memory Walls
        "warp_shared_ld_st_instructions": warp_shared_ld_st_instructions(d),
        "warp_shared_transactions": l1_shared_transactions(d),
        # Instruction mix
        **instmix(d),
        **instmix_pct(d),
        # Occupancy
        **occupancy(d),
        # Efficiency
        **efficiency(d),
        # Performance
        **performance(d, tensor_flop_factor),
    }


def mean(data):
    try:
        return statistics.geometric_mean(data)
    except statistics.StatisticsError:
        # With empty series or series with just zeros
        return 0.0


def accumulate(d):
    return {k: mean(v) for k, v in d.items()}


def average_dicts(dicts):
    totals = defaultdict(float)
    counts = defaultdict(int)
    for d in dicts:
        for k, v in d.items():
            totals[k] += v
            counts[k] += 1
    return {k: totals[k] / counts[k] for k in totals}


def parse_kernels(arg):
    return [k.strip() for k in arg.split(",") if k.strip()]


def match_kernel(name, kernels):
    return any(k in name for k in kernels)

def kernel_label(name, kernels):
    for k in kernels:
        if k in name:
            return k
    return None


def transform(f, kernels, tensor_flop_factor):
    r = csv.DictReader(f, delimiter=",", quotechar='"')
    metrics_by_kernel = defaultdict(lambda: defaultdict(list))
    kernel_order = []
    for row in r:
        kernel_name = row["Kernel Name"]
        label = kernel_label(kernel_name, kernels)
        if label is None:
            continue
        if label not in metrics_by_kernel:
            kernel_order.append(label)
        metric_name = row["Metric Name"]
        value = float(row["Metric Value"].replace(",", ""))
        metrics_by_kernel[label][metric_name].append(value)
    data_by_kernel = {}
    for kernel_name, metrics in metrics_by_kernel.items():
        data_by_kernel[kernel_name] = canonicalize(accumulate(metrics), tensor_flop_factor)
    return data_by_kernel, kernel_order


def parse_raw_metrics(f, kernels):
    metrics_by_kernel = defaultdict(lambda: defaultdict(list))
    units_by_kernel = defaultdict(dict)
    kernel_order = []
    active = False
    active_kernel = None
    active_label = None
    for line in f:
        stripped = line.strip()
        if not stripped:
            continue
        if stripped.startswith(("==PROF==", "[", "Metric Name", "---")):
            continue
        if stripped.startswith("void "):
            active_kernel = stripped
            active_label = kernel_label(stripped, kernels)
            active = active_label is not None
            if active and active_label not in metrics_by_kernel:
                kernel_order.append(active_label)
            continue
        if "__" not in stripped:
            continue
        parts = stripped.split()
        if len(parts) < 2:
            continue
        name = parts[0]
        value_str = parts[-1].replace(",", "")
        unit = parts[-2] if len(parts) > 2 else ""
        try:
            value = float(value_str)
        except ValueError:
            continue
        if active and active_label:
            metrics_by_kernel[active_label][name].append(value)
            units_by_kernel[active_label][name] = unit
    accumulated = {}
    for kernel_name, metrics in metrics_by_kernel.items():
        accumulated[kernel_name] = accumulate(metrics)
    return accumulated, units_by_kernel, kernel_order


CANONICAL_KEYS = [
    "elapsed_s",
    "dram_bytes",
    "l2_bytes",
    "l1_bytes",
    "flop",
    "scalar_flop",
    "fp_flop",
    "dp_flop",
    "tensor_flop",
    "thread_instructions",
    "l1_global_transactions",
    "l1_shared_transactions",
    "l2_transactions",
    "dram_transactions",
    "warp_instructions",
    "l1_total_32B_transactions",
    "warp_global_ld_st_instructions",
    "warp_shared_ld_st_instructions",
    "warp_shared_transactions",
    "fp_instructions",
    "int_instructions",
    "mem_instructions",
    "cf_instructions",
    "threadcomm_instructions",
    "misc_instructions",
    "tot_instructions",
    "fp_instructions_pct",
    "int_instructions_pct",
    "mem_instructions_pct",
    "cf_instructions_pct",
    "threadcomm_instructions_pct",
    "misc_instructions_pct",
    "peak_occupancy_margin_pct",
    "sustained_occupancy_pct",
    "efficiency",
    "flops",
]


def safe_div(numerator, denominator):
    if denominator == 0:
        return 0.0
    return numerator / denominator


def data_to_roofline_vars(data):
    elapsed_s = data["elapsed_s"]
    sp_perf = safe_div(data["fp_flop"], elapsed_s * 1000000000.0)
    dp_perf = safe_div(data["dp_flop"], elapsed_s * 1000000000.0)
    inst_perf = safe_div(data["thread_instructions"], elapsed_s * 1000000000.0)
    return {
        "sp_perf": sp_perf,
        "dp_perf": dp_perf,
        "inst_perf": inst_perf,
        "l1_sp_intensity": safe_div(data["fp_flop"], data["l1_bytes"]),
        "l2_sp_intensity": safe_div(data["fp_flop"], data["l2_bytes"]),
        "hbm_sp_intensity": safe_div(data["fp_flop"], data["dram_bytes"]),
        "l1_dp_intensity": safe_div(data["dp_flop"], data["l1_bytes"]),
        "l2_dp_intensity": safe_div(data["dp_flop"], data["l2_bytes"]),
        "hbm_dp_intensity": safe_div(data["dp_flop"], data["dram_bytes"]),
        "l1_inst_intensity": safe_div(
            data["thread_instructions"], data["l1_total_32B_transactions"]
        ),
        "l2_inst_intensity": safe_div(data["thread_instructions"], data["l2_transactions"]),
        "hbm_inst_intensity": safe_div(data["thread_instructions"], data["dram_transactions"]),
    }


def average_kernel_dicts(dicts, kernel_order):
    merged = {}
    for kernel in kernel_order:
        per_kernel = []
        for d in dicts:
            if kernel in d:
                per_kernel.append(d[kernel])
        if per_kernel:
            merged[kernel] = average_dicts(per_kernel)
    return merged


def kernel_point_types(count):
    points = [5, 7, 9, 11, 13, 3, 4, 6, 8, 10, 12, 14]
    return [points[i % len(points)] for i in range(count)]


def escape_gnuplot_string(value):
    return value.replace("\\", "\\\\").replace('"', '\\"')


def per_second_to_gbps(value, unit):
    unit = unit.lower()
    if unit in ("gbyte/s", "gbyte"):
        return value
    if unit in ("mbyte/s", "mbyte"):
        return value / 1000.0
    if unit in ("kbyte/s", "kbyte"):
        return value / 1000000.0
    # Assume bytes/s
    return value / 1000000000.0


def per_second_to_bytes_per_s(value, unit):
    unit = unit.lower()
    if unit in ("gbyte/s", "gbyte"):
        return value * 1000000000.0
    if unit in ("mbyte/s", "mbyte"):
        return value * 1000000.0
    if unit in ("kbyte/s", "kbyte"):
        return value * 1000.0
    return value


def to_bytes(value, unit):
    unit = unit.lower()
    if unit in ("gbyte",):
        return value * 1000000000.0
    if unit in ("mbyte",):
        return value * 1000000.0
    if unit in ("kbyte",):
        return value * 1000.0
    return value


def per_second_to_hz(value, unit):
    unit = unit.lower()
    if unit in ("ghz",):
        return value * 1000000000.0
    if unit in ("mhz",):
        return value * 1000000.0
    if unit in ("khz",):
        return value * 1000.0
    return value


def peaks_from_raw(metrics, units):
    peaks = {}
    l1_key = "derived__l1tex__lsu_writeback_bytes_mem_lg.sum.per_second"
    l2_key = "derived__lts__lts2xbar_bytes.sum.per_second"
    hbm_key = "dram__bytes.sum.per_second"
    l1_bytes_per_s = None
    l2_bytes_per_s = None
    hbm_bytes_per_s = None
    if l1_key in metrics:
        l1_bytes_per_s = per_second_to_bytes_per_s(metrics[l1_key], units.get(l1_key, ""))
        peaks["l1_peak"] = l1_bytes_per_s / 1000000000.0
    if l2_key in metrics:
        l2_bytes_per_s = per_second_to_bytes_per_s(metrics[l2_key], units.get(l2_key, ""))
        peaks["l2_peak"] = l2_bytes_per_s / 1000000000.0
    if hbm_key in metrics:
        hbm_bytes_per_s = per_second_to_bytes_per_s(metrics[hbm_key], units.get(hbm_key, ""))
        peaks["hbm_peak"] = hbm_bytes_per_s / 1000000000.0

    if l1_bytes_per_s:
        peaks["l1_peak_txn"] = (l1_bytes_per_s / 32.0) / 1000000000.0
    if l2_bytes_per_s:
        peaks["l2_peak_txn"] = (l2_bytes_per_s / 32.0) / 1000000000.0
    if hbm_bytes_per_s:
        peaks["hbm_peak_txn"] = (hbm_bytes_per_s / 32.0) / 1000000000.0

    clock_khz = metrics.get("device__attribute_clock_rate")
    if clock_khz:
        clock_hz = clock_khz * 1000.0
        sm_count = metrics.get("device__attribute_multiprocessor_count") or metrics.get(
            "launch__sm_count"
        )
        ffma_sum = "sm__sass_thread_inst_executed_op_ffma_pred_on.sum.peak_sustained"
        dfma_sum = "sm__sass_thread_inst_executed_op_dfma_pred_on.sum.peak_sustained"
        ffma_avg = "sm__sass_thread_inst_executed_op_ffma_pred_on.avg.peak_sustained"
        dfma_avg = "sm__sass_thread_inst_executed_op_dfma_pred_on.avg.peak_sustained"

        fp_inst_per_cycle = None
        dp_inst_per_cycle = None
        if ffma_sum in metrics:
            fp_inst_per_cycle = metrics[ffma_sum]
        elif sm_count and ffma_avg in metrics:
            fp_inst_per_cycle = metrics[ffma_avg] * sm_count
        if dfma_sum in metrics:
            dp_inst_per_cycle = metrics[dfma_sum]
        elif sm_count and dfma_avg in metrics:
            dp_inst_per_cycle = metrics[dfma_avg] * sm_count

        if fp_inst_per_cycle:
            peaks["peak_fp"] = fp_inst_per_cycle * clock_hz * 2.0 / 1000000000.0
            peaks["peak_nofma_fp"] = peaks["peak_fp"] / 2.0
        if dp_inst_per_cycle:
            peaks["peak_dp"] = dp_inst_per_cycle * clock_hz * 2.0 / 1000000000.0
            peaks["peak_nofma_dp"] = peaks["peak_dp"] / 2.0

        smsp_sum = "smsp__inst_executed.sum.peak_sustained"
        smsp_avg = "smsp__inst_executed.avg.peak_sustained"
        sched = metrics.get("device__attribute_num_schedulers_per_multiprocessor")
        smsp_count = sm_count * sched if sm_count and sched else None
        inst_per_cycle = None
        if smsp_sum in metrics:
            inst_per_cycle = metrics[smsp_sum]
        elif smsp_count and smsp_avg in metrics:
            inst_per_cycle = metrics[smsp_avg] * smsp_count
        if inst_per_cycle:
            peaks["inst_peak"] = inst_per_cycle * clock_hz / 1000000000.0
    return peaks


def raw_to_data(metrics, units, tensor_flop_factor):
    if "sm__cycles_elapsed.avg.per_second" in metrics:
        metrics["sm__cycles_elapsed.avg.per_second"] = per_second_to_hz(
            metrics["sm__cycles_elapsed.avg.per_second"],
            units.get("sm__cycles_elapsed.avg.per_second", ""),
        )
    if "gpu__time_duration.sum" in metrics:
        elapsed_s = metrics["gpu__time_duration.sum"] * 1e-6
        if "sm__cycles_elapsed.avg.per_second" in metrics and "sm__cycles_elapsed.avg" not in metrics:
            metrics["sm__cycles_elapsed.avg"] = (
                metrics["sm__cycles_elapsed.avg.per_second"] * elapsed_s
            )
        for key in ("dram__bytes.sum", "lts__t_bytes.sum", "l1tex__t_bytes.sum"):
            per_key = key + ".per_second"
            if per_key in metrics and key not in metrics:
                bps = per_second_to_bytes_per_s(metrics[per_key], units.get(per_key, ""))
                metrics[key] = bps * elapsed_s
    for key in ("dram__bytes.sum", "lts__t_bytes.sum", "l1tex__t_bytes.sum"):
        if key in metrics:
            metrics[key] = to_bytes(metrics[key], units.get(key, ""))

    required = [
        "sm__cycles_elapsed.avg",
        "sm__cycles_elapsed.avg.per_second",
        "dram__bytes.sum",
        "lts__t_bytes.sum",
        "l1tex__t_bytes.sum",
        "sm__sass_thread_inst_executed_op_dfma_pred_on.sum",
        "sm__sass_thread_inst_executed_op_dmul_pred_on.sum",
        "sm__sass_thread_inst_executed_op_dadd_pred_on.sum",
        "sm__sass_thread_inst_executed_op_ffma_pred_on.sum",
        "sm__sass_thread_inst_executed_op_fmul_pred_on.sum",
        "sm__sass_thread_inst_executed_op_fadd_pred_on.sum",
        "sm__sass_thread_inst_executed_op_hfma_pred_on.sum",
        "sm__sass_thread_inst_executed_op_hmul_pred_on.sum",
        "sm__sass_thread_inst_executed_op_hadd_pred_on.sum",
        "sm__inst_executed_pipe_tensor.sum",
        "sm__sass_thread_inst_executed_op_integer_pred_on.sum",
        "sm__sass_thread_inst_executed_op_control_pred_on.sum",
        "sm__sass_thread_inst_executed_op_inter_thread_communication_pred_on.sum",
        "sm__sass_thread_inst_executed_op_memory_pred_on.sum",
        "sm__sass_thread_inst_executed_op_bit_pred_on.sum",
        "sm__sass_thread_inst_executed_op_conversion_pred_on.sum",
        "sm__sass_thread_inst_executed_op_misc_pred_on.sum",
        "smsp__thread_inst_executed.sum",
        "l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum",
        "l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum",
        "l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum",
        "l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum",
        "lts__t_sectors_op_read.sum",
        "lts__t_sectors_op_atom.sum",
        "lts__t_sectors_op_red.sum",
        "lts__t_sectors_op_write.sum",
        "dram__sectors_read.sum",
        "dram__sectors_write.sum",
        "smsp__inst_executed.sum",
        "smsp__inst_executed_op_global_ld.sum",
        "smsp__inst_executed_op_shared_ld.sum",
        "smsp__inst_executed_op_shared_st.sum",
        "sm__maximum_warps_per_active_cycle_pct",
        "sm__warps_active.avg.pct_of_peak_sustained_active",
    ]
    missing = [k for k in required if k not in metrics]
    if missing:
        missing_str = ", ".join(missing)
        raise ValueError(f"Missing required raw metrics: {missing_str}")
    return canonicalize(metrics, tensor_flop_factor)


if __name__ == "__main__":
    args = parser.parse_args()
    if not (args.input or args.raw):
        parser.error("one of --input or --raw is required")
    kernels = parse_kernels(args.kernels)
    peaks = {}
    if args.input:
        data_by_kernel, kernel_order = transform(args.input, kernels, args.tensor_flop_factor)
    else:
        raw_files = args.raw or []
        if args.warmup:
            raw_files = raw_files[args.warmup :]
        if not raw_files:
            raise ValueError("No raw runs remaining after warmup")
        data_runs = []
        peaks_runs = []
        kernel_order = []
        for raw_file in raw_files:
            metrics_by_kernel, units_by_kernel, run_order = parse_raw_metrics(raw_file, kernels)
            for name in run_order:
                if name not in kernel_order:
                    kernel_order.append(name)
            if args.debug_ai_raw:
                raw_keys = [
                    "l1tex__t_bytes.sum",
                    "lts__t_bytes.sum",
                    "dram__bytes.sum",
                    "sm__sass_thread_inst_executed_op_ffma_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_fmul_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_fadd_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_hfma_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_hmul_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_hadd_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_dfma_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_dmul_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_dadd_pred_on.sum",
                    "sm__inst_executed_pipe_tensor.sum",
                ]
                for kernel_name, metrics in metrics_by_kernel.items():
                    print(f"[debug-ai-raw] {kernel_name}", file=sys.stderr)
                    for key in raw_keys:
                        if key in metrics:
                            unit = units_by_kernel[kernel_name].get(key, "")
                            print(
                                f"[debug-ai-raw] {kernel_name} {key}={metrics[key]} {unit}",
                                file=sys.stderr,
                            )
            run_data = {}
            for kernel_name, metrics in metrics_by_kernel.items():
                run_data[kernel_name] = raw_to_data(
                    metrics, units_by_kernel[kernel_name], args.tensor_flop_factor
                )
            data_runs.append(run_data)
            if run_order:
                first_kernel = run_order[0]
                peaks_runs.append(
                    peaks_from_raw(
                        metrics_by_kernel[first_kernel], units_by_kernel[first_kernel]
                    )
                )
        data_by_kernel = average_kernel_dicts(data_runs, kernel_order)
        peaks = average_dicts(peaks_runs)
    template = args.template.read()
    if not data_by_kernel:
        raise ValueError("No kernels matched the provided list")
    primary_kernel = kernel_order[0]
    primary_data = data_by_kernel[primary_kernel]
    output = template.format(**primary_data)
    instmix_lines = []
    for kernel_name in kernel_order:
        data = data_by_kernel[kernel_name]
        escaped_name = escape_gnuplot_string(kernel_name)
        instmix_lines.append(
            f"\"{escaped_name}\" {data['misc_instructions_pct']} {data['threadcomm_instructions_pct']} "
            f"{data['cf_instructions_pct']} {data['mem_instructions_pct']} "
            f"{data['int_instructions_pct']} {data['fp_instructions_pct']}"
        )
    output = output + "\n$instmix << EOD\n" + "\n".join(instmix_lines) + "\nEOD\n"
    if args.raw and args.input:
        metrics_by_kernel, units_by_kernel, run_order = parse_raw_metrics(args.raw[0], kernels)
        if run_order:
            peaks = peaks_from_raw(
                metrics_by_kernel[run_order[0]], units_by_kernel[run_order[0]]
            )
    if peaks:
        extra = "\n".join(f"{k} = {v}" for k, v in peaks.items())
        output = output + "\n" + extra
    output = output + "\n"
    output = output + f"kernel_count = {len(kernel_order)}\n"
    point_types = kernel_point_types(len(kernel_order))
    roofline_sp_points = []
    roofline_dp_points = []
    roofline_inst_points = []
    data_block = []
    data_block.append("# kernel " + " ".join(CANONICAL_KEYS))
    for idx, kernel_name in enumerate(kernel_order):
        data = data_by_kernel[kernel_name]
        if args.debug_ai:
            l1 = data.get("l1_bytes", 0.0)
            l2 = data.get("l2_bytes", 0.0)
            hbm = data.get("dram_bytes", 0.0)
            fp = data.get("fp_flop", 0.0)
            print(
                f"[debug-ai] {kernel_name} fp_flop={fp} "
                f"l1_bytes={l1} l2_bytes={l2} dram_bytes={hbm}",
                file=sys.stderr,
            )
            if l1 and l2 and hbm:
                print(
                    f"[debug-ai] {kernel_name} "
                    f"l1_ai={fp / l1} l2_ai={fp / l2} hbm_ai={fp / hbm}",
                    file=sys.stderr,
                )
        vars = data_to_roofline_vars(data)
        name_key = f"kernel_name_{idx}"
        point_key = f"kernel_point_{idx}"
        escaped_name = escape_gnuplot_string(kernel_name)
        output = output + f"{name_key} = \"{escaped_name}\"\n"
        output = output + f"{point_key} = {point_types[idx]}\n"
        for key, value in vars.items():
            output = output + f"k{idx}_{key} = {value}\n"
        roofline_sp_points.append(
            f"[0:0:1] '+' us (k{idx}_l1_sp_intensity):(k{idx}_sp_perf) "
            f"with points lc rgb l1_color pt {point_key} ps point_size title {name_key}"
        )
        roofline_sp_points.append(
            f"[0:0:1] '+' us (k{idx}_l2_sp_intensity):(k{idx}_sp_perf) "
            f"with points lc rgb l2_color pt {point_key} ps point_size notitle"
        )
        roofline_sp_points.append(
            f"[0:0:1] '+' us (k{idx}_hbm_sp_intensity):(k{idx}_sp_perf) "
            f"with points lc rgb hbm_color pt {point_key} ps point_size notitle"
        )
        roofline_dp_points.append(
            f"[0:0:1] '+' us (k{idx}_l1_dp_intensity):(k{idx}_dp_perf) "
            f"with points lc rgb l1_color pt {point_key} ps point_size title {name_key}"
        )
        roofline_dp_points.append(
            f"[0:0:1] '+' us (k{idx}_l2_dp_intensity):(k{idx}_dp_perf) "
            f"with points lc rgb l2_color pt {point_key} ps point_size notitle"
        )
        roofline_dp_points.append(
            f"[0:0:1] '+' us (k{idx}_hbm_dp_intensity):(k{idx}_dp_perf) "
            f"with points lc rgb hbm_color pt {point_key} ps point_size notitle"
        )
        roofline_inst_points.append(
            f"[0:0:1] '+' us (k{idx}_l1_inst_intensity):(k{idx}_inst_perf) "
            f"with points lc rgb l1_color pt {point_key} ps point_size title {name_key}"
        )
        roofline_inst_points.append(
            f"[0:0:1] '+' us (k{idx}_l2_inst_intensity):(k{idx}_inst_perf) "
            f"with points lc rgb l2_color pt {point_key} ps point_size notitle"
        )
        roofline_inst_points.append(
            f"[0:0:1] '+' us (k{idx}_hbm_inst_intensity):(k{idx}_inst_perf) "
            f"with points lc rgb hbm_color pt {point_key} ps point_size notitle"
        )
        row_values = [escaped_name] + [str(data.get(k, 0.0)) for k in CANONICAL_KEYS]
        quoted = [f"\"{row_values[0]}\""] + row_values[1:]
        data_block.append(" ".join(quoted))
    output = output + "\nroofline_sp_points = \"" + ", ".join(roofline_sp_points) + "\"\n"
    output = output + "\nroofline_dp_points = \"" + ", ".join(roofline_dp_points) + "\"\n"
    output = output + "\nroofline_inst_points = \"" + ", ".join(roofline_inst_points) + "\"\n"
    output = output + "\n$kernel_stats << EOD\n" + "\n".join(data_block) + "\nEOD\n"
    print(output)
    # else:
    #     print("\n".join(f"{k} = {v}" for k, v in data.items()))
