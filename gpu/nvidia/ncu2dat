#!/usr/bin/env python3

import csv
import sys
import argparse
import statistics
import math
from collections import defaultdict

parser = argparse.ArgumentParser(
    description="Generate gnuplot data file from an NVIDIA Nsight profiler CSV file. Reads the CSV from stdin and writes the result to stdout."
)
parser.add_argument(
    "--template",
    "-t",
    type=argparse.FileType("r"),
    default=None,
    help="Template file containing a python format string to be instantiated with resulting values",
)

parser.add_argument(
    "--input",
    "-i",
    type=argparse.FileType("r"),
    default=None,
    help="Input file",
)
parser.add_argument(
    "--kernels",
    "-k",
    required=True,
    help="Comma-separated list of kernel name substrings to include",
)
parser.add_argument(
    "--raw",
    action="append",
    type=argparse.FileType("r"),
    default=None,
    help="Raw NCU report from --page raw --section SpeedOfLight_HierarchicalSingleRooflineChart (repeatable)",
)
parser.add_argument(
    "--warmup",
    type=int,
    default=0,
    help="Number of initial raw runs to discard when averaging",
)
parser.add_argument(
    "--tensor-flop-factor",
    type=float,
    default=512.0,
    help="FLOPs per tensor instruction (architecture/precision dependent)",
)
parser.add_argument(
    "--debug-ai",
    action="store_true",
    help="Print per-kernel AI inputs (fp_flop, l1/l2/dram bytes) to stderr",
)
parser.add_argument(
    "--debug-ai-raw",
    action="store_true",
    help="Print raw counters used for AI/FLOP computations to stderr (raw inputs only)",
)
parser.add_argument(
    "--arch",
    type=int,
    default=None,
    help="Compute capability (e.g., 75, 80, 90) to select L1 writeback metrics",
)


def elapsed_s(d):
    return d["sm__cycles_elapsed.avg"] / float(d["sm__cycles_elapsed.avg.per_second"])


def scalar_flop(d):
    return (
        2 * d["sm__sass_thread_inst_executed_op_dfma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_dmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_dadd_pred_on.sum"]
        + 2 * d["sm__sass_thread_inst_executed_op_ffma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_fmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_fadd_pred_on.sum"]
        + 2 * d["sm__sass_thread_inst_executed_op_hfma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_hmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_hadd_pred_on.sum"]
    )


def tensor_flop(d, tensor_flop_factor):
    return tensor_flop_factor * d["sm__inst_executed_pipe_tensor.sum"]

def dp_flop(d):
    return (
        2 * d["sm__sass_thread_inst_executed_op_dfma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_dmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_dadd_pred_on.sum"]
    )


def fp_flop(d, tensor_flop_factor):
    return (
        2 * d["sm__sass_thread_inst_executed_op_ffma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_fmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_fadd_pred_on.sum"]
        + 2 * d["sm__sass_thread_inst_executed_op_hfma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_hmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_hadd_pred_on.sum"]
        + tensor_flop(d, tensor_flop_factor)
    )


def flop(d, tensor_flop_factor):
    return fp_flop(d, tensor_flop_factor) + dp_flop(d)


def thread_instructions(d):
    return d["smsp__thread_inst_executed.sum"] / 32.0


def l1_global_transactions(d):
    return (
        d["l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum"]
        + d["l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum"]
    )


def l1_shared_transactions(d):
    return (
        d["l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum"]
        + d["l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum"]
    )


def l2_transactions(d):
    return (
        d["lts__t_sectors_op_read.sum"]
        + d["lts__t_sectors_op_atom.sum"] * 2
        + d["lts__t_sectors_op_red.sum"] * 2
        + d["lts__t_sectors_op_write.sum"]
    )


def dram_transactions(d):
    return d["dram__sectors_read.sum"] + d["dram__sectors_write.sum"]


def warp_instructions(d):
    return d["smsp__inst_executed.sum"]


def l1_total_32B_transactions(d):
    return l1_global_transactions(d) + 4 * l1_shared_transactions(d)


def warp_global_ld_st_instructions(d):
    return d["smsp__inst_executed_op_global_ld.sum"]


def warp_shared_ld_st_instructions(d):
    return (
        d["smsp__inst_executed_op_shared_ld.sum"]
        + d["smsp__inst_executed_op_shared_st.sum"]
    )


def dram_bytes(d):
    return d["dram__bytes.sum"]


def l2_bytes(d):
    return d["lts__t_bytes.sum"]


def l1_bytes(d):
    return d["l1tex__t_bytes.sum"]


def fp_instructions(d):
    return (
        d["sm__sass_thread_inst_executed_op_dfma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_dmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_dadd_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_ffma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_fmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_fadd_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_hfma_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_hmul_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_hadd_pred_on.sum"]
        + d["sm__inst_executed_pipe_tensor.sum"]
    )


def int_instructions(d):
    return d["sm__sass_thread_inst_executed_op_integer_pred_on.sum"]


def cf_instructions(d):
    return d["sm__sass_thread_inst_executed_op_control_pred_on.sum"]


def threadcomm_instructions(d):
    return d["sm__sass_thread_inst_executed_op_inter_thread_communication_pred_on.sum"]


def mem_instructions(d):
    return d["sm__sass_thread_inst_executed_op_memory_pred_on.sum"]


def misc_instructions(d):
    return (
        d["sm__sass_thread_inst_executed_op_bit_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_conversion_pred_on.sum"]
        + d["sm__sass_thread_inst_executed_op_misc_pred_on.sum"]
    )


def tot_instructions(d):
    return (
        fp_instructions(d)
        + int_instructions(d)
        + cf_instructions(d)
        + threadcomm_instructions(d)
        + mem_instructions(d)
        + misc_instructions(d)
    )


def instmix_pct(d):
    tot = tot_instructions(d)
    keys = [
        "fp_instructions_pct",
        "int_instructions_pct",
        "mem_instructions_pct",
        "cf_instructions_pct",
        "threadcomm_instructions_pct",
        "misc_instructions_pct",
    ]
    values = [
        fp_instructions(d) / tot * 100,
        int_instructions(d) / tot * 100,
        mem_instructions(d) / tot * 100,
        cf_instructions(d) / tot * 100,
        threadcomm_instructions(d) / tot * 100,
        misc_instructions(d) / tot * 100,
    ]
    # Apply largest remainder method to compensate for rounding errors
    components = [math.modf(v) for v in values]
    integral = [v[1] for v in components]
    fractional = list(sorted(enumerate(v[0] for v in components), key=lambda x: x[1]))
    while sum(integral) < 100:
        idx, _ = fractional.pop()
        integral[idx] += 1
    return {keys[i]: integral[i] for i in range(len(keys))}


def instmix(d):
    return {
        "fp_instructions": fp_instructions(d),
        "int_instructions": int_instructions(d),
        "mem_instructions": mem_instructions(d),
        "cf_instructions": cf_instructions(d),
        "threadcomm_instructions": threadcomm_instructions(d),
        "misc_instructions": misc_instructions(d),
        "tot_instructions": tot_instructions(d),
    }


def occupancy(d):
    peak = d["sm__maximum_warps_per_active_cycle_pct"]
    sustained = d["sm__warps_active.avg.pct_of_peak_sustained_active"]
    return {
        "peak_occupancy_margin_pct": peak - sustained,
        "sustained_occupancy_pct": sustained,
    }


def efficiency(d):
    return {"efficiency": thread_instructions(d) / warp_instructions(d) * 100}


def performance(d, tensor_flop_factor):
    return {"flops": flop(d, tensor_flop_factor) / (elapsed_s(d) * 1000000000)}


def l1_metric_keys(arch, kind):
    if arch is None:
        return [
            f"l1tex__lsu_writeback_active_mem_lgds.sum.{kind}",
            f"l1tex__lsu_writeback_active_mem_lg.sum.{kind}",
            f"l1tex__lsu_writeback_active.sum.{kind}",
        ]
    if arch >= 90:
        return [f"l1tex__lsu_writeback_active_mem_lgds.sum.{kind}"]
    if arch >= 75:
        return [f"l1tex__lsu_writeback_active_mem_lg.sum.{kind}"]
    return [f"l1tex__lsu_writeback_active.sum.{kind}"]


def canonicalize(d, tensor_flop_factor, arch):
    sp_flops_per_s = None
    l1_bytes_per_s = None
    l2_bytes_per_s = None
    hbm_bytes_per_s = None
    if "smsp__cycles_elapsed.avg.per_second" in d:
        smsp_cycles_per_s = d["smsp__cycles_elapsed.avg.per_second"]
        sp_flops_per_cycle = (
            d.get("smsp__sass_thread_inst_executed_op_fadd_pred_on.sum.per_cycle_elapsed", 0.0)
            + d.get("smsp__sass_thread_inst_executed_op_fmul_pred_on.sum.per_cycle_elapsed", 0.0)
            + 2.0
            * d.get("smsp__sass_thread_inst_executed_op_ffma_pred_on.sum.per_cycle_elapsed", 0.0)
        )
        sp_flops_per_s = sp_flops_per_cycle * smsp_cycles_per_s
    l1_key, l1_val = pick_first(d, l1_metric_keys(arch, "per_second"))
    if l1_key:
        l1_bytes_per_s = l1_val * 128.0
    if "lts__lts2xbar_cycles_active.sum.per_second" in d:
        l2_bytes_per_s = d["lts__lts2xbar_cycles_active.sum.per_second"] * 32.0
    if "dram__bytes.sum.per_second" in d:
        hbm_bytes_per_s = d["dram__bytes.sum.per_second"]
    return {
        # Timing
        "elapsed_s": elapsed_s(d),
        # Bandwidth
        "dram_bytes": dram_bytes(d),
        "l2_bytes": l2_bytes(d),
        "l1_bytes": l1_bytes(d),
        "l1_bytes_per_s": l1_bytes_per_s,
        "l2_bytes_per_s": l2_bytes_per_s,
        "hbm_bytes_per_s": hbm_bytes_per_s,
        "sp_flops_per_s": sp_flops_per_s,
        "dp_flops_per_s": None,
        # FLOP
        "flop": flop(d, tensor_flop_factor),
        "scalar_flop": scalar_flop(d),
        "fp_flop": fp_flop(d, tensor_flop_factor),
        "dp_flop": dp_flop(d),
        "tensor_flop": tensor_flop(d, tensor_flop_factor),
        # 1. Instruction Intensity and Performance
        "thread_instructions": thread_instructions(d),
        "l1_global_transactions": l1_global_transactions(d),
        "l1_shared_transactions": l1_shared_transactions(d),
        "l2_transactions": l2_transactions(d),
        "dram_transactions": dram_transactions(d),
        # 2. Thread Predication
        "warp_instructions": warp_instructions(d),
        # 3. Global Memory Pattern Walls
        "l1_total_32B_transactions": l1_total_32B_transactions(d),
        "warp_global_ld_st_instructions": warp_global_ld_st_instructions(d),
        # 4. Shared Memory Walls
        "warp_shared_ld_st_instructions": warp_shared_ld_st_instructions(d),
        "warp_shared_transactions": l1_shared_transactions(d),
        # Instruction mix
        **instmix(d),
        **instmix_pct(d),
        # Occupancy
        **occupancy(d),
        # Efficiency
        **efficiency(d),
        # Performance
        **performance(d, tensor_flop_factor),
    }


def accumulate(d):
    return {k: sum(v) for k, v in d.items()}


def average_dicts(dicts):
    totals = defaultdict(float)
    counts = defaultdict(int)
    for d in dicts:
        for k, v in d.items():
            totals[k] += v
            counts[k] += 1
    return {k: totals[k] / counts[k] for k in totals}


def sum_dicts(dicts):
    totals = defaultdict(float)
    for d in dicts:
        for k, v in d.items():
            if v is None:
                continue
            totals[k] += v
    return dict(totals)


def parse_kernels(arg):
    return [k.strip() for k in arg.split(",") if k.strip()]


def match_kernel(name, kernels):
    return any(k in name for k in kernels)

def kernel_label(name, kernels):
    for k in kernels:
        if k in name:
            return k
    return None


def transform(f, kernels, tensor_flop_factor):
    r = csv.DictReader(f, delimiter=",", quotechar='"')
    metrics_by_kernel = defaultdict(lambda: defaultdict(list))
    kernel_order = []
    for row in r:
        kernel_name = row["Kernel Name"]
        label = kernel_label(kernel_name, kernels)
        if label is None:
            continue
        if label not in metrics_by_kernel:
            kernel_order.append(label)
        metric_name = row["Metric Name"]
        value = float(row["Metric Value"].replace(",", ""))
        metrics_by_kernel[label][metric_name].append(value)
    data_by_kernel = {}
    for kernel_name, metrics in metrics_by_kernel.items():
        data_by_kernel[kernel_name] = canonicalize(accumulate(metrics), tensor_flop_factor)
    return data_by_kernel, kernel_order


def parse_raw_metrics(f, kernels):
    metrics_by_kernel = defaultdict(lambda: defaultdict(list))
    units_by_kernel = defaultdict(dict)
    kernel_order = []
    active = False
    active_kernel = None
    active_label = None
    for line in f:
        stripped = line.strip()
        if not stripped:
            continue
        if stripped.startswith(("==PROF==", "[", "Metric Name", "---")):
            continue
        if stripped.startswith("void "):
            active_kernel = stripped
            active_label = kernel_label(stripped, kernels)
            active = active_label is not None
            if active and active_label not in metrics_by_kernel:
                kernel_order.append(active_label)
            continue
        if "__" not in stripped:
            continue
        parts = stripped.split()
        if len(parts) < 2:
            continue
        name = parts[0]
        value_str = parts[-1].replace(",", "")
        unit = parts[-2] if len(parts) > 2 else ""
        try:
            value = float(value_str)
        except ValueError:
            continue
        if active and active_label:
            metrics_by_kernel[active_label][name].append(value)
            units_by_kernel[active_label][name] = unit
    accumulated = {}
    for kernel_name, metrics in metrics_by_kernel.items():
        accumulated[kernel_name] = accumulate(metrics)
    return accumulated, units_by_kernel, kernel_order


CANONICAL_KEYS = [
    "elapsed_s",
    "dram_bytes",
    "l2_bytes",
    "l1_bytes",
    "flop",
    "scalar_flop",
    "fp_flop",
    "dp_flop",
    "tensor_flop",
    "thread_instructions",
    "l1_global_transactions",
    "l1_shared_transactions",
    "l2_transactions",
    "dram_transactions",
    "warp_instructions",
    "l1_total_32B_transactions",
    "warp_global_ld_st_instructions",
    "warp_shared_ld_st_instructions",
    "warp_shared_transactions",
    "fp_instructions",
    "int_instructions",
    "mem_instructions",
    "cf_instructions",
    "threadcomm_instructions",
    "misc_instructions",
    "tot_instructions",
    "fp_instructions_pct",
    "int_instructions_pct",
    "mem_instructions_pct",
    "cf_instructions_pct",
    "threadcomm_instructions_pct",
    "misc_instructions_pct",
    "peak_occupancy_margin_pct",
    "sustained_occupancy_pct",
    "efficiency",
    "flops",
]


def safe_div(numerator, denominator):
    if denominator == 0:
        return 0.0
    return numerator / denominator


def data_to_roofline_vars(data):
    elapsed_s = data["elapsed_s"]
    sp_flops_per_s = data.get("sp_flops_per_s")
    dp_flops_per_s = data.get("dp_flops_per_s")
    sp_perf = (
        safe_div(sp_flops_per_s, 1000000000.0)
        if sp_flops_per_s is not None
        else safe_div(data["fp_flop"], elapsed_s * 1000000000.0)
    )
    dp_perf = (
        safe_div(dp_flops_per_s, 1000000000.0)
        if dp_flops_per_s is not None
        else safe_div(data["dp_flop"], elapsed_s * 1000000000.0)
    )
    inst_perf = safe_div(data["thread_instructions"], elapsed_s * 1000000000.0)
    l1_bytes_per_s = data.get("l1_bytes_per_s")
    l2_bytes_per_s = data.get("l2_bytes_per_s")
    hbm_bytes_per_s = data.get("hbm_bytes_per_s")
    return {
        "sp_perf": sp_perf,
        "dp_perf": dp_perf,
        "inst_perf": inst_perf,
        "warp_shared_instruction_performance": safe_div(
            data["warp_shared_ld_st_instructions"], elapsed_s * 1000000000.0
        ),
        "shared_warp_inst_intensity": safe_div(
            data["warp_shared_ld_st_instructions"], data["warp_shared_transactions"]
        ),
        "l1_sp_intensity": safe_div(sp_flops_per_s, l1_bytes_per_s)
        if sp_flops_per_s is not None and l1_bytes_per_s is not None
        else safe_div(data["fp_flop"], data["l1_bytes"]),
        "l2_sp_intensity": safe_div(sp_flops_per_s, l2_bytes_per_s)
        if sp_flops_per_s is not None and l2_bytes_per_s is not None
        else safe_div(data["fp_flop"], data["l2_bytes"]),
        "hbm_sp_intensity": safe_div(sp_flops_per_s, hbm_bytes_per_s)
        if sp_flops_per_s is not None and hbm_bytes_per_s is not None
        else safe_div(data["fp_flop"], data["dram_bytes"]),
        "l1_dp_intensity": safe_div(dp_flops_per_s, l1_bytes_per_s)
        if dp_flops_per_s is not None and l1_bytes_per_s is not None
        else safe_div(data["dp_flop"], data["l1_bytes"]),
        "l2_dp_intensity": safe_div(dp_flops_per_s, l2_bytes_per_s)
        if dp_flops_per_s is not None and l2_bytes_per_s is not None
        else safe_div(data["dp_flop"], data["l2_bytes"]),
        "hbm_dp_intensity": safe_div(dp_flops_per_s, hbm_bytes_per_s)
        if dp_flops_per_s is not None and hbm_bytes_per_s is not None
        else safe_div(data["dp_flop"], data["dram_bytes"]),
        "l1_inst_intensity": safe_div(
            data["thread_instructions"], data["l1_total_32B_transactions"]
        ),
        "l2_inst_intensity": safe_div(data["thread_instructions"], data["l2_transactions"]),
        "hbm_inst_intensity": safe_div(data["thread_instructions"], data["dram_transactions"]),
    }


def merge_kernel_dicts(dicts, kernel_order):
    rate_keys = {
        "sp_flops_per_s",
        "dp_flops_per_s",
        "l1_bytes_per_s",
        "l2_bytes_per_s",
        "hbm_bytes_per_s",
        "flops",
        "efficiency",
        "peak_occupancy_margin_pct",
        "sustained_occupancy_pct",
    }
    merged = {}
    for kernel in kernel_order:
        per_kernel = []
        for d in dicts:
            if kernel in d:
                per_kernel.append(d[kernel])
        if not per_kernel:
            continue
        sums = defaultdict(float)
        counts = defaultdict(int)
        for d in per_kernel:
            for k, v in d.items():
                if v is None:
                    continue
                if k in rate_keys or k.endswith("_pct"):
                    sums[k] += v
                    counts[k] += 1
                else:
                    sums[k] += v
        merged_kernel = {}
        for k, v in sums.items():
            if k in rate_keys or k.endswith("_pct"):
                merged_kernel[k] = v / counts[k] if counts[k] else 0.0
            else:
                merged_kernel[k] = v
        merged[kernel] = merged_kernel
    return merged


def kernel_point_types(count):
    points = [5, 7, 9, 11, 13, 3, 4, 6, 8, 10, 12, 14]
    return [points[i % len(points)] for i in range(count)]


def escape_gnuplot_string(value):
    return value.replace("\\", "\\\\").replace('"', '\\"')


def per_second_to_gbps(value, unit):
    unit = unit.lower()
    if unit in ("gbyte/s", "gbyte"):
        return value
    if unit in ("mbyte/s", "mbyte"):
        return value / 1000.0
    if unit in ("kbyte/s", "kbyte"):
        return value / 1000000.0
    # Assume bytes/s
    return value / 1000000000.0


def per_second_to_bytes_per_s(value, unit):
    unit = unit.lower()
    if unit in ("gbyte/s", "gbyte"):
        return value * 1000000000.0
    if unit in ("mbyte/s", "mbyte"):
        return value * 1000000.0
    if unit in ("kbyte/s", "kbyte"):
        return value * 1000.0
    return value


def to_bytes(value, unit):
    unit = unit.lower()
    if unit in ("gbyte",):
        return value * 1000000000.0
    if unit in ("mbyte",):
        return value * 1000000.0
    if unit in ("kbyte",):
        return value * 1000.0
    return value


def per_second_to_hz(value, unit):
    unit = unit.lower()
    if unit in ("ghz", "gcycles/s", "gc/s", "gcycle/s"):
        return value * 1000000000.0 if value < 1000000.0 else value
    if unit in ("mhz", "mcycles/s", "mc/s", "mcycle/s"):
        return value * 1000000.0 if value < 1000000000.0 else value
    if unit in ("khz", "kcycles/s", "kc/s", "kcycle/s"):
        return value * 1000.0 if value < 1000000000000.0 else value
    if unit in ("hz", "cycles/s", "cycle/s"):
        return value
    return value


def pick_first(metrics, keys):
    for k in keys:
        if k in metrics:
            return k, metrics[k]
    return None, None


def peaks_from_raw(metrics, units, arch):
    def is_per_second(unit):
        unit = unit.lower()
        return "/s" in unit or unit.endswith("hz")

    peaks = {}
    hbm_peak_key = "dram__bytes.sum.peak_sustained"
    hbm_cycles_key = "dram__cycles_elapsed.avg.per_second"
    l1_cycles_key = "l1tex__cycles_elapsed.avg.per_second"
    l2_peak_key = "lts__lts2xbar_cycles_active.sum.peak_sustained"
    l2_cycles_key = "lts__cycles_elapsed.avg.per_second"

    l1_peak_keys = l1_metric_keys(arch, "peak_sustained")

    if hbm_peak_key in metrics and hbm_cycles_key in metrics:
        hbm_unit = units.get(hbm_peak_key, "")
        if is_per_second(hbm_unit):
            peaks["hbm_peak"] = per_second_to_gbps(metrics[hbm_peak_key], hbm_unit)
        else:
            hbm_cycles_per_s = per_second_to_hz(
                metrics[hbm_cycles_key], units.get(hbm_cycles_key, "")
            )
            hbm_bytes_per_cycle = metrics[hbm_peak_key]
            hbm_bytes_per_s = hbm_bytes_per_cycle * hbm_cycles_per_s
            peaks["hbm_peak"] = hbm_bytes_per_s / 1000000000.0
    l1_peak_key, l1_peak_val = pick_first(metrics, l1_peak_keys)
    if l1_peak_key and l1_cycles_key in metrics:
        l1_unit = units.get(l1_peak_key, "")
        if is_per_second(l1_unit):
            peaks["l1_peak"] = per_second_to_gbps(l1_peak_val * 128.0, l1_unit)
        else:
            l1_cycles_per_s = per_second_to_hz(
                metrics[l1_cycles_key], units.get(l1_cycles_key, "")
            )
            l1_bytes_per_s = (l1_peak_val * 128.0) * l1_cycles_per_s
            peaks["l1_peak"] = l1_bytes_per_s / 1000000000.0
    if l2_peak_key in metrics and l2_cycles_key in metrics:
        l2_unit = units.get(l2_peak_key, "")
        if is_per_second(l2_unit):
            peaks["l2_peak"] = per_second_to_gbps(metrics[l2_peak_key] * 32.0, l2_unit)
        else:
            l2_cycles_per_s = per_second_to_hz(
                metrics[l2_cycles_key], units.get(l2_cycles_key, "")
            )
            l2_bytes_per_s = (metrics[l2_peak_key] * 32.0) * l2_cycles_per_s
            peaks["l2_peak"] = l2_bytes_per_s / 1000000000.0

    if "l1_peak" in peaks:
        peaks["l1_peak_txn"] = peaks["l1_peak"] / 32.0
    if "l2_peak" in peaks:
        peaks["l2_peak_txn"] = peaks["l2_peak"] / 32.0
    if "hbm_peak" in peaks:
        peaks["hbm_peak_txn"] = peaks["hbm_peak"] / 32.0

    sp_peak_key = "sm__sass_thread_inst_executed_op_ffma_pred_on.sum.peak_sustained"
    dp_peak_key = "sm__sass_thread_inst_executed_op_dfma_pred_on.sum.peak_sustained"
    sm_cycles_key = "sm__cycles_elapsed.avg.per_second"
    if sm_cycles_key in metrics:
        sm_cycles_per_s = per_second_to_hz(
            metrics[sm_cycles_key], units.get(sm_cycles_key, "")
        )
        if sp_peak_key in metrics:
            sp_unit = units.get(sp_peak_key, "")
            if is_per_second(sp_unit):
                peaks["peak_fp"] = (
                    per_second_to_hz(metrics[sp_peak_key], sp_unit) * 2.0 / 1000000000.0
                )
            else:
                peaks["peak_fp"] = (
                    metrics[sp_peak_key] * 2.0 * sm_cycles_per_s / 1000000000.0
                )
            peaks["peak_nofma_fp"] = peaks["peak_fp"] / 2.0
        if dp_peak_key in metrics:
            dp_unit = units.get(dp_peak_key, "")
            if is_per_second(dp_unit):
                peaks["peak_dp"] = (
                    per_second_to_hz(metrics[dp_peak_key], dp_unit) * 2.0 / 1000000000.0
                )
            else:
                peaks["peak_dp"] = (
                    metrics[dp_peak_key] * 2.0 * sm_cycles_per_s / 1000000000.0
                )
            peaks["peak_nofma_dp"] = peaks["peak_dp"] / 2.0

    smsp_cycles_key = "smsp__cycles_elapsed.avg.per_second"
    smsp_sum_key = "smsp__inst_executed.sum.peak_sustained"
    smsp_avg_key = "smsp__inst_executed.avg.peak_sustained"
    if smsp_cycles_key in metrics:
        smsp_cycles_per_s = per_second_to_hz(
            metrics[smsp_cycles_key], units.get(smsp_cycles_key, "")
        )
        inst_per_cycle = None
        if smsp_sum_key in metrics:
            inst_per_cycle = metrics[smsp_sum_key]
        elif smsp_avg_key in metrics:
            sm_count = metrics.get("device__attribute_multiprocessor_count") or metrics.get(
                "launch__sm_count"
            )
            sched = metrics.get("device__attribute_num_schedulers_per_multiprocessor")
            smsp_count = sm_count * sched if sm_count and sched else None
            if smsp_count:
                inst_per_cycle = metrics[smsp_avg_key] * smsp_count
        if inst_per_cycle:
            peaks["inst_peak"] = inst_per_cycle * smsp_cycles_per_s / 1000000000.0
    return peaks


def raw_to_data(metrics, units, tensor_flop_factor, arch):
    if "sm__cycles_elapsed.avg.per_second" in metrics:
        metrics["sm__cycles_elapsed.avg.per_second"] = per_second_to_hz(
            metrics["sm__cycles_elapsed.avg.per_second"],
            units.get("sm__cycles_elapsed.avg.per_second", ""),
        )
    if "smsp__cycles_elapsed.avg.per_second" in metrics:
        metrics["smsp__cycles_elapsed.avg.per_second"] = per_second_to_hz(
            metrics["smsp__cycles_elapsed.avg.per_second"],
            units.get("smsp__cycles_elapsed.avg.per_second", ""),
        )
    for key in (
        "dram__cycles_elapsed.avg.per_second",
        "l1tex__cycles_elapsed.avg.per_second",
        "lts__cycles_elapsed.avg.per_second",
        "l1tex__lsu_writeback_active.sum.per_second",
        "l1tex__lsu_writeback_active_mem_lg.sum.per_second",
        "l1tex__lsu_writeback_active_mem_lgds.sum.per_second",
        "lts__lts2xbar_cycles_active.sum.per_second",
    ):
        if key in metrics:
            metrics[key] = per_second_to_hz(metrics[key], units.get(key, ""))
    if "dram__bytes.sum.per_second" in metrics:
        metrics["dram__bytes.sum.per_second"] = per_second_to_bytes_per_s(
            metrics["dram__bytes.sum.per_second"],
            units.get("dram__bytes.sum.per_second", ""),
        )
    if "gpu__time_duration.sum" in metrics:
        elapsed_s = metrics["gpu__time_duration.sum"] * 1e-6
        if "sm__cycles_elapsed.avg.per_second" in metrics and "sm__cycles_elapsed.avg" not in metrics:
            metrics["sm__cycles_elapsed.avg"] = (
                metrics["sm__cycles_elapsed.avg.per_second"] * elapsed_s
            )
        for key in ("dram__bytes.sum", "lts__t_bytes.sum", "l1tex__t_bytes.sum"):
            per_key = key + ".per_second"
            if per_key in metrics and key not in metrics:
                bps = per_second_to_bytes_per_s(metrics[per_key], units.get(per_key, ""))
                metrics[key] = bps * elapsed_s
    for key in ("dram__bytes.sum", "lts__t_bytes.sum", "l1tex__t_bytes.sum"):
        if key in metrics:
            metrics[key] = to_bytes(metrics[key], units.get(key, ""))

    required = [
        "sm__cycles_elapsed.avg",
        "sm__cycles_elapsed.avg.per_second",
        "dram__bytes.sum",
        "lts__t_bytes.sum",
        "l1tex__t_bytes.sum",
        "smsp__cycles_elapsed.avg.per_second",
        "smsp__sass_thread_inst_executed_op_fadd_pred_on.sum.per_cycle_elapsed",
        "smsp__sass_thread_inst_executed_op_fmul_pred_on.sum.per_cycle_elapsed",
        "smsp__sass_thread_inst_executed_op_ffma_pred_on.sum.per_cycle_elapsed",
        "lts__lts2xbar_cycles_active.sum.per_second",
        "dram__bytes.sum.per_second",
        "dram__bytes.sum.peak_sustained",
        "dram__cycles_elapsed.avg.per_second",
        # L1 per-second and peak variants are handled below (need at least one)
        "l1tex__cycles_elapsed.avg.per_second",
        "lts__lts2xbar_cycles_active.sum.peak_sustained",
        "lts__cycles_elapsed.avg.per_second",
        "sm__sass_thread_inst_executed_op_ffma_pred_on.sum.peak_sustained",
        "sm__sass_thread_inst_executed_op_dfma_pred_on.sum",
        "sm__sass_thread_inst_executed_op_dmul_pred_on.sum",
        "sm__sass_thread_inst_executed_op_dadd_pred_on.sum",
        "sm__sass_thread_inst_executed_op_ffma_pred_on.sum",
        "sm__sass_thread_inst_executed_op_fmul_pred_on.sum",
        "sm__sass_thread_inst_executed_op_fadd_pred_on.sum",
        "sm__sass_thread_inst_executed_op_hfma_pred_on.sum",
        "sm__sass_thread_inst_executed_op_hmul_pred_on.sum",
        "sm__sass_thread_inst_executed_op_hadd_pred_on.sum",
        "sm__inst_executed_pipe_tensor.sum",
        "sm__sass_thread_inst_executed_op_integer_pred_on.sum",
        "sm__sass_thread_inst_executed_op_control_pred_on.sum",
        "sm__sass_thread_inst_executed_op_inter_thread_communication_pred_on.sum",
        "sm__sass_thread_inst_executed_op_memory_pred_on.sum",
        "sm__sass_thread_inst_executed_op_bit_pred_on.sum",
        "sm__sass_thread_inst_executed_op_conversion_pred_on.sum",
        "sm__sass_thread_inst_executed_op_misc_pred_on.sum",
        "smsp__thread_inst_executed.sum",
        "l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum",
        "l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum",
        "l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum",
        "l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum",
        "lts__t_sectors_op_read.sum",
        "lts__t_sectors_op_atom.sum",
        "lts__t_sectors_op_red.sum",
        "lts__t_sectors_op_write.sum",
        "dram__sectors_read.sum",
        "dram__sectors_write.sum",
        "smsp__inst_executed.sum",
        "smsp__inst_executed_op_global_ld.sum",
        "smsp__inst_executed_op_shared_ld.sum",
        "smsp__inst_executed_op_shared_st.sum",
        "sm__maximum_warps_per_active_cycle_pct",
        "sm__warps_active.avg.pct_of_peak_sustained_active",
    ]
    missing = [k for k in required if k not in metrics]
    l1_per_s_keys = l1_metric_keys(arch, "per_second")
    l1_peak_keys = l1_metric_keys(arch, "peak_sustained")
    if arch is None:
        if not any(k in metrics for k in l1_per_s_keys):
            missing.append("l1tex__lsu_writeback_active*.sum.per_second")
        if not any(k in metrics for k in l1_peak_keys):
            missing.append("l1tex__lsu_writeback_active*.sum.peak_sustained")
    else:
        for k in l1_per_s_keys:
            if k not in metrics:
                missing.append(k)
        for k in l1_peak_keys:
            if k not in metrics:
                missing.append(k)
    if missing:
        missing_str = ", ".join(missing)
        raise ValueError(f"Missing required raw metrics: {missing_str}")
    return canonicalize(metrics, tensor_flop_factor, arch)


if __name__ == "__main__":
    args = parser.parse_args()
    if not (args.input or args.raw):
        parser.error("one of --input or --raw is required")
    kernels = parse_kernels(args.kernels)
    peaks = {}
    if args.input:
        data_by_kernel, kernel_order = transform(args.input, kernels, args.tensor_flop_factor)
    else:
        raw_files = args.raw or []
        if args.warmup:
            raw_files = raw_files[args.warmup :]
        if not raw_files:
            raise ValueError("No raw runs remaining after warmup")
        data_runs = []
        peaks_runs = []
        kernel_order = []
        for raw_file in raw_files:
            metrics_by_kernel, units_by_kernel, run_order = parse_raw_metrics(raw_file, kernels)
            for name in run_order:
                if name not in kernel_order:
                    kernel_order.append(name)
            if args.debug_ai_raw:
                raw_keys = [
                    "l1tex__t_bytes.sum",
                    "lts__t_bytes.sum",
                    "dram__bytes.sum",
                    "sm__sass_thread_inst_executed_op_ffma_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_fmul_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_fadd_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_hfma_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_hmul_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_hadd_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_dfma_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_dmul_pred_on.sum",
                    "sm__sass_thread_inst_executed_op_dadd_pred_on.sum",
                    "sm__inst_executed_pipe_tensor.sum",
                ]
                for kernel_name, metrics in metrics_by_kernel.items():
                    print(f"[debug-ai-raw] {kernel_name}", file=sys.stderr)
                    for key in raw_keys:
                        if key in metrics:
                            unit = units_by_kernel[kernel_name].get(key, "")
                            print(
                                f"[debug-ai-raw] {kernel_name} {key}={metrics[key]} {unit}",
                                file=sys.stderr,
                            )
            run_data = {}
            for kernel_name, metrics in metrics_by_kernel.items():
                run_data[kernel_name] = raw_to_data(
                    metrics, units_by_kernel[kernel_name], args.tensor_flop_factor, args.arch
                )
            data_runs.append(run_data)
            if run_order:
                first_kernel = run_order[0]
                peaks_runs.append(
                    peaks_from_raw(
                        metrics_by_kernel[first_kernel],
                        units_by_kernel[first_kernel],
                        args.arch,
                    )
                )
        data_by_kernel = merge_kernel_dicts(data_runs, kernel_order)
        peaks = average_dicts(peaks_runs)
    template = args.template.read()
    if not data_by_kernel:
        raise ValueError("No kernels matched the provided list")
    primary_kernel = kernel_order[0]
    primary_data = data_by_kernel[primary_kernel]
    output = template.format(**primary_data)
    instmix_lines = []
    for kernel_name in kernel_order:
        data = data_by_kernel[kernel_name]
        escaped_name = escape_gnuplot_string(kernel_name)
        instmix_lines.append(
            f"\"{escaped_name}\" {data['misc_instructions_pct']} {data['threadcomm_instructions_pct']} "
            f"{data['cf_instructions_pct']} {data['mem_instructions_pct']} "
            f"{data['int_instructions_pct']} {data['fp_instructions_pct']}"
        )
    output = output + "\n$instmix << EOD\n" + "\n".join(instmix_lines) + "\nEOD\n"
    if args.raw and args.input:
        metrics_by_kernel, units_by_kernel, run_order = parse_raw_metrics(args.raw[0], kernels)
        if run_order:
            peaks = peaks_from_raw(
                metrics_by_kernel[run_order[0]],
                units_by_kernel[run_order[0]],
                args.arch,
            )
    if peaks:
        extra = "\n".join(f"{k} = {v}" for k, v in peaks.items())
        output = output + "\n" + extra
    output = output + "\n"
    output = output + f"kernel_count = {len(kernel_order)}\n"
    point_types = kernel_point_types(len(kernel_order))
    roofline_sp_points = []
    roofline_dp_points = []
    roofline_inst_points = []
    roofline_shared_points = []
    data_block = []
    data_block.append("# kernel " + " ".join(CANONICAL_KEYS))
    occupancy_block = []
    occupancy_block.append("# kernel sustained_occupancy_pct peak_occupancy_margin_pct")
    predication_block = []
    predication_block.append("# kernel efficiency")
    for idx, kernel_name in enumerate(kernel_order):
        data = data_by_kernel[kernel_name]
        if args.debug_ai:
            l1 = data.get("l1_bytes", 0.0)
            l2 = data.get("l2_bytes", 0.0)
            hbm = data.get("dram_bytes", 0.0)
            fp = data.get("fp_flop", 0.0)
            print(
                f"[debug-ai] {kernel_name} fp_flop={fp} "
                f"l1_bytes={l1} l2_bytes={l2} dram_bytes={hbm}",
                file=sys.stderr,
            )
            if l1 and l2 and hbm:
                print(
                    f"[debug-ai] {kernel_name} "
                    f"l1_ai={fp / l1} l2_ai={fp / l2} hbm_ai={fp / hbm}",
                    file=sys.stderr,
                )
        vars = data_to_roofline_vars(data)
        name_key = f"kernel_name_{idx}"
        point_key = f"kernel_point_{idx}"
        escaped_name = escape_gnuplot_string(kernel_name)
        output = output + f"{name_key} = \"{escaped_name}\"\n"
        output = output + f"{point_key} = {point_types[idx]}\n"
        for key, value in vars.items():
            output = output + f"k{idx}_{key} = {value}\n"
        roofline_sp_points.append(
            f"[0:0:1] '+' us (k{idx}_l1_sp_intensity):(k{idx}_sp_perf) "
            f"with points lc rgb l1_color pt {point_key} ps point_size title {name_key}"
        )
        roofline_sp_points.append(
            f"[0:0:1] '+' us (k{idx}_l2_sp_intensity):(k{idx}_sp_perf) "
            f"with points lc rgb l2_color pt {point_key} ps point_size notitle"
        )
        roofline_sp_points.append(
            f"[0:0:1] '+' us (k{idx}_hbm_sp_intensity):(k{idx}_sp_perf) "
            f"with points lc rgb hbm_color pt {point_key} ps point_size notitle"
        )
        roofline_dp_points.append(
            f"[0:0:1] '+' us (k{idx}_l1_dp_intensity):(k{idx}_dp_perf) "
            f"with points lc rgb l1_color pt {point_key} ps point_size title {name_key}"
        )
        roofline_dp_points.append(
            f"[0:0:1] '+' us (k{idx}_l2_dp_intensity):(k{idx}_dp_perf) "
            f"with points lc rgb l2_color pt {point_key} ps point_size notitle"
        )
        roofline_dp_points.append(
            f"[0:0:1] '+' us (k{idx}_hbm_dp_intensity):(k{idx}_dp_perf) "
            f"with points lc rgb hbm_color pt {point_key} ps point_size notitle"
        )
        roofline_inst_points.append(
            f"[0:0:1] '+' us (k{idx}_l1_inst_intensity):(k{idx}_inst_perf) "
            f"with points lc rgb l1_color pt {point_key} ps point_size title {name_key}"
        )
        roofline_inst_points.append(
            f"[0:0:1] '+' us (k{idx}_l2_inst_intensity):(k{idx}_inst_perf) "
            f"with points lc rgb l2_color pt {point_key} ps point_size notitle"
        )
        roofline_inst_points.append(
            f"[0:0:1] '+' us (k{idx}_hbm_inst_intensity):(k{idx}_inst_perf) "
            f"with points lc rgb hbm_color pt {point_key} ps point_size notitle"
        )
        roofline_shared_points.append(
            f"[0:0:1] '+' us (k{idx}_shared_warp_inst_intensity):(k{idx}_warp_shared_instruction_performance) "
            f"with points lc rgb shared_color pt {point_key} ps point_size title {name_key}"
        )
        row_values = [escaped_name] + [str(data.get(k, 0.0)) for k in CANONICAL_KEYS]
        quoted = [f"\"{row_values[0]}\""] + row_values[1:]
        data_block.append(" ".join(quoted))
        occupancy_block.append(
            f"\"{escaped_name}\" {data.get('sustained_occupancy_pct', 0.0)} "
            f"{data.get('peak_occupancy_margin_pct', 0.0)}"
        )
        predication_block.append(f"\"{escaped_name}\" {data.get('efficiency', 0.0)}")
    output = output + "\nroofline_sp_points = \"" + ", ".join(roofline_sp_points) + "\"\n"
    output = output + "\nroofline_dp_points = \"" + ", ".join(roofline_dp_points) + "\"\n"
    output = output + "\nroofline_inst_points = \"" + ", ".join(roofline_inst_points) + "\"\n"
    output = output + "\nroofline_shared_points = \"" + ", ".join(roofline_shared_points) + "\"\n"
    output = output + "\n$occupancy << EOD\n" + "\n".join(occupancy_block) + "\nEOD\n"
    output = output + "\n$predication << EOD\n" + "\n".join(predication_block) + "\nEOD\n"
    output = output + "\n$kernel_stats << EOD\n" + "\n".join(data_block) + "\nEOD\n"
    print(output)
    # else:
    #     print("\n".join(f"{k} = {v}" for k, v in data.items()))
